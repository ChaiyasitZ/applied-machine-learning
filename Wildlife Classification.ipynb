{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow matplotlib seaborn numpy pandas scikit-learn pillow flask flask-cors pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.11.7)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Image augmentation and normalization\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255, rotation_range=30, zoom_range=0.2, \n",
    "    horizontal_flip=True, width_shift_range=0.2, height_shift_range=0.2\n",
    ")\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load dataset\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    \"wildlife_dataset/train\", target_size=(224, 224), batch_size=64, class_mode='categorical'\n",
    ")\n",
    "val_data = val_test_datagen.flow_from_directory(\n",
    "    \"wildlife_dataset/val\", target_size=(224, 224), batch_size=64, class_mode='categorical'\n",
    ")\n",
    "test_data = val_test_datagen.flow_from_directory(\n",
    "    \"wildlife_dataset/test\", target_size=(224, 224), batch_size=64, class_mode='categorical', shuffle=False\n",
    ")\n",
    "\n",
    "# Print the number of images in each dataset\n",
    "print(f\"Number of training images: {train_data.samples}\")\n",
    "print(f\"Number of validation images: {val_data.samples}\")\n",
    "print(f\"Number of test images: {test_data.samples}\")\n",
    "\n",
    "# Automatically detect the number of classes\n",
    "NUM_CLASSES = len(train_data.class_indices)\n",
    "class_labels = list(train_data.class_indices.keys())\n",
    "print(f\"Detected {NUM_CLASSES} classes:\", train_data.class_indices)\n",
    "\n",
    "# Load MobileNetV2 with pre-trained ImageNet weights\n",
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze pre-trained layers\n",
    "\n",
    "# Add custom layers on top\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "output_layer = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=base_model.input, outputs=output_layer)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(\"best_wildlife_classifier.h5\", save_best_only=True, monitor=\"val_accuracy\", mode=\"max\")\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_data, validation_data=val_data, epochs=12, callbacks=callbacks)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "test_data.reset()\n",
    "Y_pred = model.predict(test_data)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "y_true = test_data.classes\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(y_true, y_pred, target_names=test_data.class_indices.keys())\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Save final model\n",
    "model.save(\"wildlife_classifier.h5\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training & Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, jsonify\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "from pyngrok import ngrok\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"wildlife_classifier.h5\")\n",
    "\n",
    "# Get class names from your dataset directory\n",
    "# You'll need to adjust this path to match your actual dataset location\n",
    "class_names = sorted(os.listdir(\"wildlife_dataset/train\"))\n",
    "print(f\"Loaded classes: {class_names}\")\n",
    "\n",
    "def preprocess_image(image_bytes):\n",
    "    \"\"\"Process uploaded image bytes for model prediction\"\"\"\n",
    "    img = Image.open(io.BytesIO(image_bytes))\n",
    "    img = img.resize((224, 224))  # Resize to match model's expected input\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = img_array / 255.0  # Normalize\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    return img_array\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({'error': 'No file part'})\n",
    "\n",
    "    file = request.files['file']\n",
    "    if file.filename == '':\n",
    "        return jsonify({'error': 'No selected file'})\n",
    "\n",
    "    if file:\n",
    "        # Read and preprocess the image\n",
    "        img_bytes = file.read()\n",
    "        img_array = preprocess_image(img_bytes)\n",
    "\n",
    "        # Make prediction\n",
    "        predictions = model.predict(img_array)\n",
    "        predicted_class_index = np.argmax(predictions[0])\n",
    "        predicted_class = class_names[predicted_class_index]\n",
    "        confidence = float(predictions[0][predicted_class_index])\n",
    "\n",
    "        # Get top 3 predictions for display\n",
    "        top_indices = predictions[0].argsort()[-3:][::-1]\n",
    "        top_results = [\n",
    "            {\"class\": class_names[i], \"confidence\": float(predictions[0][i])}\n",
    "            for i in top_indices\n",
    "        ]\n",
    "\n",
    "        return jsonify({\n",
    "            'prediction': predicted_class,\n",
    "            'confidence': confidence,\n",
    "            'top_results': top_results\n",
    "        })\n",
    "\n",
    "def run_with_ngrok():\n",
    "    # Set up ngrok\n",
    "    port = 5000\n",
    "    public_url = ngrok.connect(port).public_url\n",
    "    print(f\" * Running on {public_url}\")\n",
    "    app.config[\"BASE_URL\"] = public_url\n",
    "    \n",
    "    # Start Flask app\n",
    "    app.run(debug=False, port=port)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_with_ngrok()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
